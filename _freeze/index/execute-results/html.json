{
  "hash": "5077989ef5cbad4ffd8a1d79ce8f727e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Functional Phytoplankton Group Retrieval From Space\nauthor:\n  - name: Susanne E. Craig\n    orcid: 0000-0002-0760-5497\n    corresponding: true\n    email: steve@curvenote.com\n    roles:\n      - Investigation\n      - Project administration\n    affiliations:\n      - NASA\n  - name: Erdem M. Karaköylü\n    orcid: 0000-0002-7859-8394\n    corresponding: true\n    email: erdemk@protonmail.com\n    roles:\n      - Investigation\n      - Software\n      - Visualization\n    affiliations:\n      - Consultant\nkeywords:\n  - Phytoplankton Functional Groups (PFGs)\n  - Hyperspectral Ocean Color\n  - Remote Sensing of Phytoplankton\n  - Chlorophyll-a Retrieval\n  - PACE Mission\n\nabstract: |\n  Phytoplankton functional groups (PFGs) play a key role in ocean biogeochemical cycling, climate regulation, and marine ecosystem dynamics. Accurate quantification of these groups from satellite ocean color data remains challenging due to spectral similarities among phytoplankton types and the limitations of existing empirical and semi-analytical models. In this study, we developed an XGBoost-based regression model to retrieve multiple PFGs and total chlorophyll-a concentrations from simulated hyperspectral remote sensing reflectance (Rrs) data, mimicking the NASA Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) mission. We also included ancillary environmental information. The final model, validated on an out-of-sample set, demonstrated strong predictive performance across most functional groups, with R² values exceeding 0.95. Dinoflagellate retrievals showed lower accuracy (R² = 0.53). Further analysis revealed that temperature was a key predictor alongside hyperspectral reflectance, suggesting that integrating external temperature data could enhance future retrieval models. Furthermore, despite using only 10% of the available hyperspectral bands, feature importance analysis showed that specific spectral regions disproportionately contributed to model predictions. These findings highlight the potential of machine learning for phytoplankton classification and inform future algorithm development for hyperspectral ocean color missions.\n\ndate: last-modified\nbibliography: references.bib\ncitation:\n  container-title: Earth and Space Science\nnumber-sections: true\njupyter: python3\n---\n\n\n## Introduction\n\nPhytoplankton functional groups (PFGs) play a fundamental role in marine biogeochemical cycles, influencing carbon sequestration, nutrient fluxes, and global climate feedbacks. Different functional groups contribute uniquely to these processes; for example, diatoms facilitate carbon export through rapid sinking, cyanobacteria fix atmospheric nitrogen, and coccolithophores regulate carbonate chemistry via calcification [@Maranon2015; @Boyd2010]. Identifying and quantifying these groups from space is crucial for understanding their ecological functions, detecting environmental changes, and improving ocean biogeochemical models [@Bopp2005; @Laufkotter2015]. However, current satellite ocean-color products primarily provide total chlorophyll-*a* concentrations, which do not directly indicate community composition. To address this gap, various remote sensing algorithms have been developed to infer phytoplankton diversity, each with limitations in distinguishing certain groups and quantifying their biomass accurately [@Mouw2017].\n\n### Remote Sensing Approaches for PFG Retrieval\n\nPhytoplankton classification from satellite remote sensing has traditionally relied on empirical and semi-analytical methods. Empirical band-ratio techniques, such as PHYSAT, classify dominant phytoplankton groups based on anomalies in spectral reflectance but are often region-specific and limited to broad functional classes [@Alvain2005; @Alvain2008]. Semi-analytical models, in contrast, use inherent optical properties (IOPs) to infer phytoplankton composition from satellite reflectance, providing a more mechanistic approach [@Hirata2011]. Hybrid models incorporate additional environmental variables, such as sea surface temperature and total chlorophyll, to infer community structure [@Brewin2010].\n\nMore recently, hyperspectral ocean-color sensors, such as NASA’s upcoming Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) mission, have been designed to improve PFG retrieval by capturing finer spectral features associated with phytoplankton pigments [@Dierssen2023]. These advancements offer the potential for enhanced classification, yet challenges remain. Optical similarity between different groups, depth-related biases in surface measurements, and uncertainties in algorithm parameterization continue to limit retrieval accuracy [@IOCCG2014]. Most current models either estimate phytoplankton size classes or assign a single dominant group per pixel, often failing to capture the complexity of mixed communities [@Ciotti2002].\n\n### Study Contribution and Approach\n\nIn this study, we introduce **extreme gradient boosting (XGBoost)** as a novel approach for retrieving phytoplankton functional groups from satellite ocean color data. XGBoost is a scalable ensemble learning algorithm that has demonstrated high performance in complex classification tasks but has not yet been widely applied to PFG retrieval [@Chen2016]. Unlike traditional empirical and semi-analytical models, our method leverages a large dataset of simulated hyperspectral reflectance and environmental variables to improve both the discrimination of functional groups and the quantification of their biomass. Previous remote sensing algorithms often classified only a dominant PFG or broad size class [@Mouw2017] and relied on empirical band relationships that lacked generalizability [@Hirata2011]. By utilizing a machine learning framework capable of integrating multiple features, our approach reduces classification errors and enhances retrieval precision.\n\nRecent studies have demonstrated the potential of XGBoost in related applications, such as harmful algal bloom detection [@Izadi2021] and phytoplankton biomass estimation [@Yan2025], highlighting its suitability for remote sensing applications. Our work aligns with the objectives of the PACE mission by contributing an advanced classification algorithm that enhances hyperspectral monitoring of phytoplankton diversity [@Zhang2024]. To our knowledge, this is the first application of XGBoost for PFG classification in ocean color remote sensing, offering a robust alternative to traditional retrieval methods.\n\n\n## Methods\n\n### Data Preparation and Feature Selection\n\nWe utilized a simulated dataset representing the world ocean over 31 days, corresponding to December 2021. The simulation generated hyperspectral remote sensing reflectance (Rrs) data, emulating a sensor configuration akin to that of the PACE instrument. Due to the high dimensionality of the original spectral data, we conducted an initial exploratory analysis and observed strong correlations among many of the channels. To reduce redundancy while preserving essential spectral information, we retained 51 channels by selecting one channel every ten. We opted against applying principal component analysis (PCA) because preliminary investigations indicated that PCA tended to overemphasize the signal from extensive blue water areas, which could mask coastal processes of interest. In addition to these 51 spectral channels, we included auxiliary environmental variables such as temperature and latitude.\n\nThe dataset was divided into training and test sets using an 80/20 split. The training set was exclusively used for model development and hyperparameter optimization, while the test set was reserved for the final validation of model performance.\n\n### Modeling and Hyperparameter Optimization\n\nWe employed an XGBoost Regressor model wrapped within a multi-output regression structure to simultaneously predict multiple phytoplankton functional groups as well as total chlorophyll-*a* concentration. XGBoost is a high-performance, scalable implementation of gradient boosting that has become a popular choice for a wide range of regression and classification tasks [@Chen2016]. It builds an ensemble of decision trees sequentially, where each new tree attempts to correct the errors made by the previous trees. By optimizing a regularized objective function, XGBoost effectively controls overfitting while enhancing prediction accuracy. Its efficient handling of sparse data, support for parallel computation, and flexible regularization mechanisms make it particularly well-suited for complex modeling tasks, such as predicting multiple phytoplankton functional groups and total chlorophyll-*a* concentration from hyperspectral and ancillary oceanographic data.\n\nGiven the complexity of the problem and the high dimensionality of the input features, it was critical to optimize the hyperparameters to achieve robust performance and prevent overfitting. To this end, we conducted hyperparameter optimization using a Bayesian optimization approach[@Snoek2012] implemented with Optuna[@Akiba2019]. The objective function minimized the root mean squared error (RMSE) computed via three-fold cross-validation on the training set. The hyperparameters under investigation included the learning rate, maximum tree depth, number of estimators, subsample ratio, column subsample ratio, and gamma (the minimum loss reduction required to make a further partition on a leaf node). The Bayesian optimization procedure allowed us to efficiently explore the hyperparameter space by leveraging past trial information to prune unpromising candidates early, thereby reducing overall computational cost.\n\n### Model Evaluation and eXplainable AI (XAI)\nOnce the optimal hyperparameter combination was identified, we retrained the final XGBoost model on the full training set using these optimized settings. Finally, we evaluated the performance of the retrained model on the held-out test set to assess its generalizability.\n\n\n#### XAI with SHAP\n\nTo enhance interpretability and gain insights into how different input features influence model predictions, we employed **Shapley Additive Explanations (SHAP)**, a widely used explainable AI (XAI) framework for interpreting complex machine learning models. SHAP is named after the concept of Shapley values, which consists in assigning importance values to each input feature by estimating its contribution to the model's predictions across different samples. The method is rooted in cooperative game theory, ensuring a fair distribution of importance scores among features [@Lundberg2017].\n\nGiven the computational complexity of our XGBoost model and the high dimensionality of the dataset, we conducted SHAP analysis on a **random subsample of 1,000 observations from the test set**. This subset was selected to balance computational feasibility while maintaining a representative sample of phytoplankton spectral diversity.\n\nWe generated **SHAP summary plots**, which provide a comprehensive visualization of feature importance and the directionality of their influence on model outputs. These plots display the magnitude of each feature's impact across all predictions, helping to identify the most influential spectral and environmental variables in determining phytoplankton functional group composition. The insights gained from SHAP analysis aid in validating model behavior and ensuring its ecological plausibility.\n\n\n##### Code Availability\nAll code used in this study is publicly available on GitHub ([GitHub repository URL]).\n\n\n## Results\n\n### Hyperparameter Optimization (HPO)\n\nWe performed hyperparameter optimization using a Bayesian optimization framework implemented with Optuna. The metric used for optimization was the average RMSE (in units of $mgL^{-1} Chl_a$ ) computed over the cross-validation folds and across all target compartments. The “full HPO run” best parameters indicate a relatively aggressive model, characterized by deep trees with many estimators, a moderate learning rate, and little regularization via gamma.\n\nThe best trial finished with an RMSE of $0.116mgL^{-1} Chl_a$. Below is the list of hyperparameters researched, the optimal values found, and an interpretation of these values:\n\n*  Learning Rate (learning_rate): $0.083$ - This moderate learning rate suggests the model takes reasonably sized steps when updating that are neither too aggressive (which might lead to overshooting the optimum) nor too conservative (which could slow down convergence). \n\n* Max Depth (max_depth): $10$ - A depth of 10 allows the trees to capture complex interactions. This may indicate that the data has non-linear relationships that benefit from deeper trees. Such a depth can be associated with overfitting. The cross-validation process during HPO should minimize this however.\n\n* Number of Estimators (n_estimators): $466$ -Building around 466 trees indicates the ensemble haa to tackle inherent complexity in the data that was not apparetn during the Exploratory Data Analysis phase. A larger number of trees generally improves performance—up to a point before overfitting becomse a risk.  This number in conjunction with the cross validation process suggest this number strikes a balance between performance and overfitting.\n\n* Subsampling (subsample): $0.658$ - This indicates each of the 466 trees is using roughly 66% of the data. This introduces randomness that helps prevent overfitting as not all samples in any cross-validation fold are used to build every tree.\n\n* Features used per tree (colsample_bytree): $0.894$ - Using about 89% of the features per tree indicates that most features are informative, and the model is allowed to consider almost the full feature set at each split. - See features used in the Methods section.\n    \n* Gamma (gamma): $8.63e-06$ - An extremely low gamma value means that almost no minimum loss reduction is required to make a split. This implies that the algorithm will split more readily, potentially capturing fine details. Awareness of this hyperparameter values is important as low gamma  can risk overfitting.\n\n\n### Optimized Model Validation\n\nThe next step was to load the best set of hyperparameter (listed above) into the model and retrain the model on the entire training set. The optimized and trained model was then validated using the test set, which prior to the HPO process and until this step had been set aside.\n\n:::{#fig-gof}\n\n![](images/goodness-of-fit_rrs_env.png)\n\nGoodness-of-fit plots for all groups and total chorophyll *a*, measured on out-of-sample data set. THe model is able to predict with very good accuracy. Dinoflagellates are the notable exception. \n:::\n\nA more complete set of metrics are summarized in table [@tbl-metrics] See further below for metrics explanation.\n\n\n| Metric        | Diatom  | Chloroph. | Cyanobac | Coccolith. | Dinoflag. | Phaeo  | Tot. Chl_a |\n|---------------|--------:|----------:|---------:|-----------:|----------:|-------:|------------|\n| **MSE**       | 0.00034 | 0.00010   | 2.89e-06 | 8.59e-05   | 1.96e-05  | 0.00011 | 0.000193  |\n| **RMSE**      | 0.0184  | 0.0100    | 0.0017   | 0.00927    | 0.00443   | 0.0105  | 0.0139    |\n| **MAE**       | 0.00878 | 0.0042    | 0.00078  | 0.0042     | 0.000637  | 0.00313 | 0.00728   |\n| **R-squared** | 0.979   | 0.958     | 0.996    | 0.985      | 0.530     | 0.999   | 0.999     |\n| **MAE/StDev** | 0.0691  | 0.0858    | 0.0302   | 0.0563     | 0.0986    | 0.00754 | 0.0182    |\n\n: Performance metrics of optimized and trained model on hold out set {#tbl-metrics}\n\n#### Explanation of metrics\n\n- **Mean Squared Error (MSE):**  \n  MSE is the average of the squared differences between the predicted and true values. Squaring the errors emphasizes larger deviations, making MSE sensitive to outliers. In our context, MSE is expressed in units of (mg L$^{-1}$ Chl$_a$)$^2$. Lower MSE values indicate better model performance.\n\n- **Root Mean Squared Error (RMSE):**  \n  RMSE is the square root of the MSE, bringing the error metric back to the original units (mg L$^{-1}$ Chl$_a$). It provides a direct measure of the average prediction error magnitude. Lower RMSE values suggest that the model’s predictions are closer to the true values.\n\n- **Mean Absolute Error (MAE):**  \n  MAE calculates the average absolute difference between predicted and true values. Unlike MSE, it does not square the errors, so it is less sensitive to large outliers. MAE is also expressed in the same units as the target variable (mg L$^{-1}$ Chl$_a$). A lower MAE indicates better predictive accuracy.\n\n- **Coefficient of Determination (R-squared):**  \n  R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where a value closer to 1 indicates that the model explains a high proportion of the variance in the data. In our results, high R-squared values generally indicate strong model performance, although lower values (e.g., for dinoflagellates) suggest room for improvement.\n\n- **MAE/StDev$_{true}$:**  \n  This ratio compares the mean absolute error to the standard deviation of the true values. It provides a relative measure of error by indicating how the average error compares to the inherent variability in the data. A lower ratio implies that the model’s prediction error is small relative to the natural variability of the observations.\n\n\n### XAI with Shapley Values\n\nThe SHAP summary plots provides insights into feature importance and their effects on model predictions for phytoplankton functional groups. Features are ranked by impact, with the x-axis representing SHAP values—positive values increase predicted chlorophyll concentration, while negative values decrease it. The violin plot format shows the distribution of SHAP values, with wider sections indicating greater variability. The color gradient represents feature values, with red for high values and blue for low values. The midpoint of the color bar reflects a percentile-based central value, not necessarily the mean, median, or mode, as it depends on the feature’s distribution.\n\n\n\n::: {#fig-shap layout-ncol=2}\n\n![](images/shap_rrs_env_dia.png){#fig-diatoms}\n\n![](images/shap_rrs_env_chl.png){#fig-chloro}\n\n![](images/shap_rrs_env_cya.png){#fig-cyano}\n\n![](images/shap_rrs_env_coc.png){#fig-coccos}\n\n![](images/shap_rrs_env_din.png){#fig-dinos}\n\n![](images/shap_rrs_env_pha.png){#fig-phaeo}\n\n![](images/shap_rrs_env_tot_cphyl.png){#fig-tot_chl}\n\nShapley values for each target variable.\n\n:::\n## Conclusion\n\n## References {.unnumbered}\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}